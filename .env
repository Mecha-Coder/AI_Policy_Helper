# Vector Database
QDRANT__SERVICE__GRPC_PORT=6334

# AI (Ollama)
NVIDIA_VISIBLE_DEVICES=all

# Frontend
NEXT_PUBLIC_API_BASE=http://localhost:8000


# Backend

VECTOR_STORE=qdrant # qdrant | memory
STORE_HOST=http://qdrant:6333
COLLECTION_NAME=policy_helper
CHUNK_SIZE=40
CHUNK_OVERLAP=5

DATA_DIR=/app/data

LLM_PROVIDER=ollama     # options: stub | openai | ollama
OLLAMA_EMBED=nomic-embed-text
OLLAMA_LLM=qwen2.5:3b-instruct-q4_K_M     # OLd: llama3.2:3b

# Note: To test offline mode, remove OLLAMA_HOST
# This will force the application to use stub LLM provider


OLLAMA_HOST=http://ollama:11434
