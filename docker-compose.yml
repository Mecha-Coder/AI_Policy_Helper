
services:
  qdrant:
    build: ./qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 5s
      timeout: 3s
      retries: 20
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334

  ollama:
    build: ./ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_LLM=llama3.2:3b
      - OLLAMA_EMBED=nomic-embed-text
      - NVIDIA_VISIBLE_DEVICES=all
    restart: unless-stopped
    runtime: nvidia

  backend:
    build: ./backend
    environment:
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-local-384}
      - LLM_PROVIDER=${LLM_PROVIDER:-stub}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      - VECTOR_STORE=${VECTOR_STORE:-qdrant}
      - COLLECTION_NAME=${COLLECTION_NAME:-policy_helper}
      - CHUNK_SIZE=${CHUNK_SIZE:-50}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-10}
      - DATA_DIR=${DATA_DIR}
    volumes:
      - ./data:/app/data:ro
    ports:
      - "8000:8000"
    depends_on:
      qdrant:
        condition: service_healthy

  frontend:
    build: ./frontend
    environment:
      - NEXT_PUBLIC_API_BASE=${NEXT_PUBLIC_API_BASE:-http://localhost:8000}
    ports:
      - "3000:3000"
    depends_on:
      - backend

volumes:
  qdrant_data:
  ollama_data:
